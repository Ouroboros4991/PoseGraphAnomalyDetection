{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba1fde-9178-40e5-90b6-3e07bc47f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scripts standaridizes the data layout of UBNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8026fa-4132-44e5-ab09-cecdcdc113d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3d2f06-548c-4293-9a48-5a27d45ca096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(\n",
    "   command: str,\n",
    "   cwd: str,\n",
    "   return_output: bool = False,\n",
    "   strip_response: bool = True,\n",
    ") -> list:\n",
    "   \"\"\"Prints output of subprocess command in real-time.\n",
    "\n",
    "\n",
    "   Args:\n",
    "       command (string): The command to run\n",
    "       return_output (bool): Return output of command as an array or not.\n",
    "       strip_response (bool): Whether or not to strip whitespaces from response.\n",
    "\n",
    "\n",
    "   Returns:\n",
    "       list|int|None: The output of the command\n",
    "   \"\"\"\n",
    "   output = []\n",
    "   with subprocess.Popen(\n",
    "       command,\n",
    "       cwd=cwd,\n",
    "       stdout=subprocess.PIPE,\n",
    "       shell=True,\n",
    "   ) as process:  # nosec\n",
    "       while True:\n",
    "           response = process.stdout.readline()\n",
    "           if response == b\"\" and process.poll() is not None:\n",
    "               break\n",
    "           if response:\n",
    "               cleaned_output = response.strip() if strip_response else response\n",
    "               if return_output:\n",
    "                   output.append(cleaned_output.decode(\"UTF-8\"))\n",
    "       command_response = process.poll()\n",
    "   return output if return_output else command_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b7fd0-1b1e-4dd4-a931-ec8c19828288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy videos\n",
    "video_source_dir = \"/mnt/c/Users/ruben/Downloads/UBnormal_data\"\n",
    "video_dest_dir = './data/UBnormal/videos'\n",
    "pathlib.Path(video_dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "videos = []\n",
    "for root, dirs, files in os.walk(video_source_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root,file)\n",
    "        if file.endswith('.mp4'):\n",
    "            videos.append(file_path)\n",
    "for file in videos:\n",
    "    file_name = file.split('/')[-1]\n",
    "    dest_name = os.path.join(video_dest_dir, file_name)\n",
    "    # shutil.copyfile(file, dest_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7847ee-4f42-4e4b-93c9-d21aaf2b6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ground truth annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134c4642-8c0e-47f3-a6d1-9b36269e79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = './raw_data/UBnormal/annotations_stg'\n",
    "pose_dir =  f'./raw_data/UBnormal/poses_stg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0019558-0a94-44fb-8e96-6e39564779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame level annotatations:\n",
    "annotations_dir = f'{annotations_folder}/frame_level'\n",
    "pathlib.Path(annotations_dir).mkdir(parents=True, exist_ok=True)\n",
    "for dataset_type in ['training', 'validation', 'test']:\n",
    "    annotations = []\n",
    "    source_dir = f\"/mnt/d/VUB/CurrentTrendsOfAI/PoseGraphAnomalyDetection/UBnormal/data/frame_gt/{dataset_type}\"\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root,file)\n",
    "            if file.endswith('.txt'):\n",
    "                annotations.append(file_path)\n",
    "    for annotation_file in annotations:\n",
    "        video_name = annotation_file.split('/')[-2]\n",
    "        with open(annotation_file) as f:\n",
    "            annotation_data = [int(float(line.strip())) for line in f.readlines()]\n",
    "        np.save(file=f'{annotations_dir}/{video_name}.npy', arr=np.array(annotation_data), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0f935-88f0-4733-87d5-5f6bc9039835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d148f3-1d44-4bda-abac-f3efe2502c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f90c2e-4c67-4535-abcd-cd95f39d89c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98261105-f137-4711-bf79-755838f4c2da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pose level annotatations\n",
    "def get_pose_data(pose_data: str) -> dict:\n",
    "    \"\"\"Load in the pose data into a dict.\n",
    "    This dict has as key the frame in which the poses where detected\n",
    "    and as values as list of each detected obj_id + the bounding box\n",
    "    \"\"\"\n",
    "    # Convert pose data so that you can fetch the poses found in a frame\n",
    "    parsed_pose_data = {}\n",
    "    for obj_id, frame_data in pose_data.items():\n",
    "        for frame_id, kp_data in frame_data.items():\n",
    "            max_x = 0\n",
    "            min_x = 10000\n",
    "            max_y = 0\n",
    "            min_y = 10000\n",
    "            \n",
    "            # Format of keypoints is x,y,confidence * 17\n",
    "            for i in range(0, len(kp_data['keypoints']), 3):\n",
    "                co_x = kp_data['keypoints'][i]\n",
    "                co_y = kp_data['keypoints'][i+1]\n",
    "                # We don't need the confidence, \n",
    "                if co_x > max_x:\n",
    "                    max_x = co_x\n",
    "                elif co_x < min_x:\n",
    "                    min_x = co_x\n",
    "                if co_y > max_y:\n",
    "                    max_y = co_y\n",
    "                elif co_y < min_y:\n",
    "                    min_y = co_y\n",
    "            if not parsed_pose_data.get(frame_id):\n",
    "                parsed_pose_data[frame_id] = []\n",
    "            nose_coordinates = (kp_data['keypoints'][0], kp_data['keypoints'][1])\n",
    "            parsed_pose_data[frame_id].append(\n",
    "                (obj_id, [min_x, min_y, max_x, max_y], nose_coordinates)\n",
    "            )\n",
    "    return parsed_pose_data\n",
    "\n",
    "# Load pose files in a dict for easy retrieval\n",
    "poses = {}\n",
    "for root, dirs, files in os.walk(pose_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root,file)\n",
    "        if file.endswith('_tracked_person.json'):\n",
    "            video_name = '_'.join(file.split('_')[:-3])\n",
    "            poses[video_name] = file_path\n",
    "\n",
    "frame_level_annotations_dir = f'{annotations_folder}/frame_level'\n",
    "annotations_dir = f'{annotations_folder}/pose_level'\n",
    "pathlib.Path(annotations_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annotations = []\n",
    "\n",
    "bounding_boxes = {}\n",
    "\n",
    "for dataset_type in ['training', 'validation', 'test']:\n",
    "    source_dir = f\"/mnt/d/VUB/CurrentTrendsOfAI/PoseGraphAnomalyDetection/UBnormal/data/frame_gt/{dataset_type}\"\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root,file)\n",
    "            if file.endswith('.txt'):\n",
    "                video_name = file.split('.')[0]\n",
    "                annotations.append(file_path)\n",
    "    bounding_box_dir = f\"/mnt/d/VUB/CurrentTrendsOfAI/PoseGraphAnomalyDetection/UBnormal/data/bounding_boxes/{dataset_type}\"\n",
    "    for root, dirs, files in os.walk(bounding_box_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root,file)\n",
    "            if file.endswith('.txt'):\n",
    "                video_name = file.split('.')[0]\n",
    "                bounding_boxes[video_name] = file_path\n",
    "\n",
    "for annotation_file in annotations:\n",
    "    video_name = annotation_file.split('/')[-2]\n",
    "    try:\n",
    "        bounding_box_file = bounding_boxes[video_name]\n",
    "        with open(bounding_box_file) as f:            # Format: [new_track_id, frame_idx, bbox.x_min, bbox.y_min, bbox.x_max, bbox.y_max\n",
    "            bounding_box_data = [[int(float(i)) for i in line.strip().split(',')]\n",
    "                                 for line in f.readlines()]\n",
    "    except KeyError as e:\n",
    "        # Normal videos don't have anomalies and hence no bounding boxes\n",
    "        if not video_name.startswith('normal'):\n",
    "            raise e\n",
    "        bounding_box_data = []\n",
    "    # Map annotation objects to tracked poses\n",
    "    with open(poses[video_name]) as f:\n",
    "        tracking_data = json.load(f)\n",
    "    pose_data = get_pose_data(tracking_data)\n",
    "    pose_gt_dict = {}\n",
    "    for bb_record in bounding_box_data:\n",
    "        new_track_id, frame_idx, annon_bb_x_min, annon_bb_y_min, annon_bb_x_max, annon_bb_y_max = bb_record\n",
    "        padded_frame_id = str(frame_idx).rjust(4, '0')\n",
    "        found_obj_id = None\n",
    "        min_distance = 100\n",
    "        # It's possible that AlphaPose does not detect the pose in a specific frame\n",
    "        for obj_id, obj_bb, nose_coordinates in pose_data.get(padded_frame_id, []):\n",
    "            obj_bb_min_x, obj_bb_min_y, obj_bb_max_x, obj_bb_max_y = obj_bb\n",
    "            if obj_bb_min_x < nose_coordinates[0] < obj_bb_max_x and obj_bb_min_y < nose_coordinates[1] < obj_bb_max_y:\n",
    "                metric = (abs(annon_bb_x_min - obj_bb_min_x)\n",
    "                          + abs(annon_bb_y_min - obj_bb_min_y)\n",
    "                          + abs(annon_bb_x_max - obj_bb_max_x)\n",
    "                          + abs(annon_bb_y_max - obj_bb_max_y))\n",
    "                if metric < min_distance:\n",
    "                    min_distance = metric\n",
    "                    found_obj_id = obj_id\n",
    "        if found_obj_id:\n",
    "            if found_obj_id not in pose_gt_dict:\n",
    "                pose_gt_dict[found_obj_id] = []\n",
    "            pose_gt_dict[found_obj_id].append(frame_idx)\n",
    "    video_length = len(np.load(f'{frame_level_annotations_dir}/{video_name}.npy', allow_pickle=True))\n",
    "    for track_id in tracking_data:\n",
    "        gt_array = [0] * video_length\n",
    "        for frame in pose_gt_dict.get(track_id, []):\n",
    "            gt_array[frame] = 1\n",
    "        pathlib.Path(f'{annotations_dir}/{video_name}').mkdir(parents=True, exist_ok=True)\n",
    "        np.save(file=f'{annotations_dir}/{video_name}/{track_id}.npy', arr=np.array(gt_array), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e767f5f-6389-4f39-854c-b2830e664ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42243d15-b17e-42aa-a04d-4a6994315430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a0f70-c2d6-49ce-b5d6-9346bb58a0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aeddb2-3703-4f89-87eb-b83483df1715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272239ea-ebd5-4b90-80ba-a81533a5b870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
